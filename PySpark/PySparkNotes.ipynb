{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e85940",
   "metadata": {},
   "source": [
    "# Getting Started with PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4002b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\pyspark\\spark\\spark-3.5.1-bin-hadoop3\\python (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\akshay\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f793e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ece59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a83af",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c21c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c6574",
   "metadata": {},
   "source": [
    "### Multiple Ways To Create Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b797d9",
   "metadata": {},
   "source": [
    "We can create the Data Frame using tuple & dictionary exploring the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9918b4",
   "metadata": {},
   "source": [
    "### 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee17eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Amarnath|\n",
      "+---+--------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Akshay'), (2,'Amarnath')]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "demo_df_01 = spark.createDataFrame(data, schema)\n",
    "\n",
    "demo_df_01.show()\n",
    "\n",
    "demo_df_01.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87ccac",
   "metadata": {},
   "source": [
    "### 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978b3549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Amarnath|\n",
      "+---+--------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{'id' : 1, 'name':'Akshay'}, {'id' : 2, 'name' :'Amarnath'}]\n",
    "\n",
    "demo_df_02 = spark.createDataFrame(data)\n",
    "\n",
    "demo_df_02.show()\n",
    "\n",
    "demo_df_02.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e074429",
   "metadata": {},
   "source": [
    "### 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b1966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Amarnath|\n",
      "+---+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [{'id' : 1, 'name':'Akshay'}, {'id' : 2, 'name' :'Amarnath'}]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name = 'id', dataType = IntegerType()),\n",
    "     StructField(name = 'name', dataType = StringType())\n",
    "])\n",
    "\n",
    "demo_df_03 = spark.createDataFrame(data, schema)\n",
    "\n",
    "demo_df_03.show()\n",
    "\n",
    "demo_df_03.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2d855",
   "metadata": {},
   "source": [
    "### Reading CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e730103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"Sample Data\\Sample_CSV_Data_01.csv\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90355a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+\n",
      "|  Name|Age|Salary|      Dept|\n",
      "+------+---+------+----------+\n",
      "|Akshay| 23| 50000|        IT|\n",
      "|  Amar| 18| 10000|        BE|\n",
      "|Adarsh| 17|  5000|   Diploma|\n",
      "|Ganesh| 16|  1000|Highschool|\n",
      "|  Alok| 11|   500|    School|\n",
      "+------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19f0e7",
   "metadata": {},
   "source": [
    "### Exploring CSV options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c7b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without inferSchema option I can see all the column data type as a String\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd6bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"Sample Data\\Sample_CSV_Data_01.csv\", header = 'true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aff2cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With inferSchema option I can see all the column data type as expected\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db286e65",
   "metadata": {},
   "source": [
    "### Select statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975a3c0",
   "metadata": {},
   "source": [
    "There are multiple ways to access the values from Data Frame exploring the same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e59b3b",
   "metadata": {},
   "source": [
    "#### Method 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea2dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_col = df.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda18e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Akshay|\n",
      "|  Amar|\n",
      "|Adarsh|\n",
      "|Ganesh|\n",
      "|  Alok|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82106bc",
   "metadata": {},
   "source": [
    "#### Method 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ef5c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Akshay|\n",
      "|  Amar|\n",
      "|Adarsh|\n",
      "|Ganesh|\n",
      "|  Alok|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single_col = df.select(df['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0742d3c",
   "metadata": {},
   "source": [
    "#### Method 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c128c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Akshay|\n",
      "|  Amar|\n",
      "|Adarsh|\n",
      "|Ganesh|\n",
      "|  Alok|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single_col = df.select(df.Name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf0236",
   "metadata": {},
   "source": [
    "### Column Renaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d73ed8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename_col = df.withColumnRenamed('Name', 'Brothers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9d28bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+----------+\n",
      "|Brothers|Age|Salary|      Dept|\n",
      "+--------+---+------+----------+\n",
      "|  Akshay| 23| 50000|        IT|\n",
      "|    Amar| 18| 10000|        BE|\n",
      "|  Adarsh| 17|  5000|   Diploma|\n",
      "|  Ganesh| 16|  1000|Highschool|\n",
      "|    Alok| 11|   500|    School|\n",
      "+--------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rename_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ded9cd",
   "metadata": {},
   "source": [
    "### Adding New column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359f8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_col = df.withColumn('SalaryIncrement',df['Salary']+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a275bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+---------------+\n",
      "|  Name|Age|Salary|      Dept|SalaryIncrement|\n",
      "+------+---+------+----------+---------------+\n",
      "|Akshay| 23| 50000|        IT|          51000|\n",
      "|  Amar| 18| 10000|        BE|          11000|\n",
      "|Adarsh| 17|  5000|   Diploma|           6000|\n",
      "|Ganesh| 16|  1000|Highschool|           2000|\n",
      "|  Alok| 11|   500|    School|           1500|\n",
      "+------+---+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e88ebff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+\n",
      "|  Name|Age|Salary|      Dept|\n",
      "+------+---+------+----------+\n",
      "|Akshay| 23| 50000|        IT|\n",
      "|  Amar| 18| 10000|        BE|\n",
      "|Adarsh| 17|  5000|   Diploma|\n",
      "|Ganesh| 16|  1000|Highschool|\n",
      "|  Alok| 11|   500|    School|\n",
      "+------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6935e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+---------------+-------------+\n",
      "|  Name|Age|Salary|      Dept|SalaryIncrement|Copied_column|\n",
      "+------+---+------+----------+---------------+-------------+\n",
      "|Akshay| 23| 50000|        IT|          51000|        50000|\n",
      "|  Amar| 18| 10000|        BE|          11000|        10000|\n",
      "|Adarsh| 17|  5000|   Diploma|           6000|         5000|\n",
      "|Ganesh| 16|  1000|Highschool|           2000|         1000|\n",
      "|  Alok| 11|   500|    School|           1500|          500|\n",
      "+------+---+------+----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "cop_df = df_new_col.withColumn('Copied_column', col('Salary'))\n",
    "\n",
    "cop_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b91ec",
   "metadata": {},
   "source": [
    "### Dropping the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "401a27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+---------------+\n",
      "|  Name|Age|Salary|      Dept|SalaryIncrement|\n",
      "+------+---+------+----------+---------------+\n",
      "|Akshay| 23| 50000|        IT|          51000|\n",
      "|  Amar| 18| 10000|        BE|          11000|\n",
      "|Adarsh| 17|  5000|   Diploma|           6000|\n",
      "|Ganesh| 16|  1000|Highschool|           2000|\n",
      "|  Alok| 11|   500|    School|           1500|\n",
      "+------+---+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b021a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_col = df_new_col.drop('SalaryIncrement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d06958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+\n",
      "|  Name|Age|Salary|      Dept|\n",
      "+------+---+------+----------+\n",
      "|Akshay| 23| 50000|        IT|\n",
      "|  Amar| 18| 10000|        BE|\n",
      "|Adarsh| 17|  5000|   Diploma|\n",
      "|Ganesh| 16|  1000|Highschool|\n",
      "|  Alok| 11|   500|    School|\n",
      "+------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3da49a",
   "metadata": {},
   "source": [
    "Note: \n",
    "1. Adding new column or dropping column or Renaming column is not a in-place operation that means these operation won't get effect to the org DF \n",
    "2. If you want to make changes in org DF you should assign the changes to the org DF variable for Example see above operation i.e. df_new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129703a3",
   "metadata": {},
   "source": [
    "## Handling Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "607b2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_prac = \\\n",
    "spark.read \\\n",
    ".csv(r\"Sample Data\\Sample_CSV_Data_02.csv\", header = 'true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afbea453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------+------+\n",
      "|    Name| Age|         City|Salary|\n",
      "+--------+----+-------------+------+\n",
      "|    John|  30|     New York| 70000|\n",
      "|    Anna|  35|San Francisco| 80000|\n",
      "|  Robert|  40|  Los Angeles| 90000|\n",
      "|   Julia|  25|      Chicago| 60000|\n",
      "| Michael|  45|      Houston|100000|\n",
      "|   Emily|  28|       Boston| 65000|\n",
      "|   David|  33|      Seattle| 75000|\n",
      "|  Akshay|NULL|       Sidnal|  NULL|\n",
      "|    NULL|NULL|         NULL|100000|\n",
      "|Amarnath|  18|        Akkol|  NULL|\n",
      "+--------+----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f0391",
   "metadata": {},
   "source": [
    "### Remove All The Null Values From The DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9e35d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+------+\n",
      "|   Name|Age|         City|Salary|\n",
      "+-------+---+-------------+------+\n",
      "|   John| 30|     New York| 70000|\n",
      "|   Anna| 35|San Francisco| 80000|\n",
      "| Robert| 40|  Los Angeles| 90000|\n",
      "|  Julia| 25|      Chicago| 60000|\n",
      "|Michael| 45|      Houston|100000|\n",
      "|  Emily| 28|       Boston| 65000|\n",
      "|  David| 33|      Seattle| 75000|\n",
      "+-------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9cb35",
   "metadata": {},
   "source": [
    "### Exploring Drop Function Parameters\n",
    "- how\n",
    "- thresh\n",
    "- subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378f0d7",
   "metadata": {},
   "source": [
    "### 1. How\n",
    "\n",
    "It has two options \n",
    "- any : this will drop the perticular row if one of the column consist of Null value\n",
    "- all : this will drop the row if all the raw data is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e66b6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+------+\n",
      "|   Name|Age|         City|Salary|\n",
      "+-------+---+-------------+------+\n",
      "|   John| 30|     New York| 70000|\n",
      "|   Anna| 35|San Francisco| 80000|\n",
      "| Robert| 40|  Los Angeles| 90000|\n",
      "|  Julia| 25|      Chicago| 60000|\n",
      "|Michael| 45|      Houston|100000|\n",
      "|  Emily| 28|       Boston| 65000|\n",
      "|  David| 33|      Seattle| 75000|\n",
      "+-------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop(how = \"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0421852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------+------+\n",
      "|    Name| Age|         City|Salary|\n",
      "+--------+----+-------------+------+\n",
      "|    John|  30|     New York| 70000|\n",
      "|    Anna|  35|San Francisco| 80000|\n",
      "|  Robert|  40|  Los Angeles| 90000|\n",
      "|   Julia|  25|      Chicago| 60000|\n",
      "| Michael|  45|      Houston|100000|\n",
      "|   Emily|  28|       Boston| 65000|\n",
      "|   David|  33|      Seattle| 75000|\n",
      "|  Akshay|NULL|       Sidnal|  NULL|\n",
      "|    NULL|NULL|         NULL|100000|\n",
      "|Amarnath|  18|        Akkol|  NULL|\n",
      "+--------+----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop(how = \"all\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75395fe",
   "metadata": {},
   "source": [
    "### 2. Threshold\n",
    "\n",
    "- This will delete the row based on minimum threshold value mentioned that is Non null value count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b0cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------+------+\n",
      "|    Name| Age|         City|Salary|\n",
      "+--------+----+-------------+------+\n",
      "|    John|  30|     New York| 70000|\n",
      "|    Anna|  35|San Francisco| 80000|\n",
      "|  Robert|  40|  Los Angeles| 90000|\n",
      "|   Julia|  25|      Chicago| 60000|\n",
      "| Michael|  45|      Houston|100000|\n",
      "|   Emily|  28|       Boston| 65000|\n",
      "|   David|  33|      Seattle| 75000|\n",
      "|  Akshay|NULL|       Sidnal|  NULL|\n",
      "|Amarnath|  18|        Akkol|  NULL|\n",
      "+--------+----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop(how = \"any\", thresh = 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a05857",
   "metadata": {},
   "source": [
    "Explanation : As you can see, rows where a non-null value appeared in only one column, with all other columns being null, were deleted based on a threshold value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91f50e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------+------+\n",
      "|    Name|Age|         City|Salary|\n",
      "+--------+---+-------------+------+\n",
      "|    John| 30|     New York| 70000|\n",
      "|    Anna| 35|San Francisco| 80000|\n",
      "|  Robert| 40|  Los Angeles| 90000|\n",
      "|   Julia| 25|      Chicago| 60000|\n",
      "| Michael| 45|      Houston|100000|\n",
      "|   Emily| 28|       Boston| 65000|\n",
      "|   David| 33|      Seattle| 75000|\n",
      "|Amarnath| 18|        Akkol|  NULL|\n",
      "+--------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop(how = \"any\", thresh = 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4e03b",
   "metadata": {},
   "source": [
    "Explanation : As you can see, rows where a non-null value appeared in one and two column, with all other columns being null, were deleted based on a threshold value of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d6c68",
   "metadata": {},
   "source": [
    "### 3. Subset\n",
    "\n",
    "- This will remove rows from a specific column where a null value is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07ac20fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+------+\n",
      "|   Name|Age|         City|Salary|\n",
      "+-------+---+-------------+------+\n",
      "|   John| 30|     New York| 70000|\n",
      "|   Anna| 35|San Francisco| 80000|\n",
      "| Robert| 40|  Los Angeles| 90000|\n",
      "|  Julia| 25|      Chicago| 60000|\n",
      "|Michael| 45|      Houston|100000|\n",
      "|  Emily| 28|       Boston| 65000|\n",
      "|  David| 33|      Seattle| 75000|\n",
      "+-------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_prac.na.drop(how = \"any\", subset=['City', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbeb0a",
   "metadata": {},
   "source": [
    "## Filter \n",
    "\n",
    "- ==\n",
    "- &\n",
    "- |\n",
    "- ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "747ef4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_prac = df_null_prac.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208d8700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+------+\n",
      "|   Name|Age|         City|Salary|\n",
      "+-------+---+-------------+------+\n",
      "|   John| 30|     New York| 70000|\n",
      "|   Anna| 35|San Francisco| 80000|\n",
      "| Robert| 40|  Los Angeles| 90000|\n",
      "|  Julia| 25|      Chicago| 60000|\n",
      "|Michael| 45|      Houston|100000|\n",
      "|  Emily| 28|       Boston| 65000|\n",
      "|  David| 33|      Seattle| 75000|\n",
      "+-------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_prac.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b59ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+------+\n",
      "| Name|Age|    City|Salary|\n",
      "+-----+---+--------+------+\n",
      "| John| 30|New York| 70000|\n",
      "|Julia| 25| Chicago| 60000|\n",
      "|Emily| 28|  Boston| 65000|\n",
      "+-----+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_prac.filter(df_filter_prac['Age']<=30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "476fc198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+------+\n",
      "| Name|Age|   City|Salary|\n",
      "+-----+---+-------+------+\n",
      "|Julia| 25|Chicago| 60000|\n",
      "+-----+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_prac.filter((df_filter_prac['Age']<=30) & (df_filter_prac['Salary']<=60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a98b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+------+\n",
      "| Name|Age|    City|Salary|\n",
      "+-----+---+--------+------+\n",
      "| John| 30|New York| 70000|\n",
      "|Julia| 25| Chicago| 60000|\n",
      "|Emily| 28|  Boston| 65000|\n",
      "+-----+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_prac.filter((df_filter_prac['Age']<=30) | (df_filter_prac['Salary']<=60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cac9af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+------+\n",
      "|   Name|Age|         City|Salary|\n",
      "+-------+---+-------------+------+\n",
      "|   Anna| 35|San Francisco| 80000|\n",
      "| Robert| 40|  Los Angeles| 90000|\n",
      "|Michael| 45|      Houston|100000|\n",
      "|  David| 33|      Seattle| 75000|\n",
      "+-------+---+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_prac.filter(~(df_filter_prac['Age']<=30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a7f0",
   "metadata": {},
   "source": [
    "## Group By & Aggregating Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48b434",
   "metadata": {},
   "source": [
    "1. Group by function will group the result based on column you pass and on top of it we can use the Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0283e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_prac = spark.read.csv(r\"Sample Data\\Sample_CSV_Data_03.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ce33a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------+------+\n",
      "|    Name| Age|         City|Salary|\n",
      "+--------+----+-------------+------+\n",
      "|    John|  30|     New York| 70000|\n",
      "|    Anna|  35|San Francisco| 80000|\n",
      "|  Robert|  40|     New York| 90000|\n",
      "|   Julia|  25|      Chicago| 60000|\n",
      "| Michael|  45|      Houston|100000|\n",
      "|   Emily|  28|       Boston| 65000|\n",
      "|   David|  33|      Seattle| 75000|\n",
      "|  Akshay|NULL|      Chicago|  NULL|\n",
      "|    Anna|  30|      Chicago|100000|\n",
      "|Amarnath|  18|      Houston|  2000|\n",
      "|  Robert|  25|     New York| 80000|\n",
      "+--------+----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baa6d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         City|sum(Salary)|\n",
      "+-------------+-----------+\n",
      "|San Francisco|      80000|\n",
      "|      Chicago|     160000|\n",
      "|      Seattle|      75000|\n",
      "|      Houston|     102000|\n",
      "|     New York|     240000|\n",
      "|       Boston|      65000|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69e8c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         City|max(Salary)|\n",
      "+-------------+-----------+\n",
      "|San Francisco|      80000|\n",
      "|      Chicago|     100000|\n",
      "|      Seattle|      75000|\n",
      "|      Houston|     100000|\n",
      "|     New York|      90000|\n",
      "|       Boston|      65000|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').max('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ad1c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         City|min(Salary)|\n",
      "+-------------+-----------+\n",
      "|San Francisco|      80000|\n",
      "|      Chicago|      60000|\n",
      "|      Seattle|      75000|\n",
      "|      Houston|       2000|\n",
      "|     New York|      70000|\n",
      "|       Boston|      65000|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').min('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61a86958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         City|avg(Salary)|\n",
      "+-------------+-----------+\n",
      "|San Francisco|    80000.0|\n",
      "|      Chicago|    80000.0|\n",
      "|      Seattle|    75000.0|\n",
      "|      Houston|    51000.0|\n",
      "|     New York|    80000.0|\n",
      "|       Boston|    65000.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').mean('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ad82dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         City|avg(Salary)|\n",
      "+-------------+-----------+\n",
      "|San Francisco|    80000.0|\n",
      "|      Chicago|    80000.0|\n",
      "|      Seattle|    75000.0|\n",
      "|      Houston|    51000.0|\n",
      "|     New York|    80000.0|\n",
      "|       Boston|    65000.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').avg('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f968ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         City|count|\n",
      "+-------------+-----+\n",
      "|San Francisco|    1|\n",
      "|      Chicago|    3|\n",
      "|      Seattle|    1|\n",
      "|      Houston|    2|\n",
      "|     New York|    3|\n",
      "|       Boston|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_prac.groupBy('City').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e95ac",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Where condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3d59ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_where_prac = df_agg_prac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c406c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_where_prac.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9c89391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    Name|    city|\n",
      "+--------+--------+\n",
      "|   Julia| Chicago|\n",
      "|   Emily|  Boston|\n",
      "|Amarnath| Houston|\n",
      "|  Robert|New York|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df_where_prac.select('Name', 'city').where(\"Age<30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39ab8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------+\n",
      "|  Name| Age|   City|Salary|\n",
      "+------+----+-------+------+\n",
      "| Julia|  25|Chicago| 60000|\n",
      "|Akshay|NULL|Chicago|  NULL|\n",
      "|  Anna|  30|Chicago|100000|\n",
      "+------+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_where_prac.where(df_where_prac.City=='Chicago').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287164c",
   "metadata": {},
   "source": [
    "### Array Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8e3da",
   "metadata": {},
   "source": [
    "Let's explore the Array data type and some functions of it\n",
    "\n",
    "1. Split - This function convert the delimeterd string into Array type\n",
    "2. Array contains - This functions check the value which is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83b3fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| SubId|\n",
      "+---+------+\n",
      "|  A|[1, 2]|\n",
      "|  B|[3, 4]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "data_array = [('A', [1,2]), ('B', [3,4])]\n",
    "\n",
    "schema_array = StructType([\n",
    "    StructField(name = 'id', dataType = StringType()),\n",
    "     StructField(name = 'SubId', dataType = ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "df_array = spark.createDataFrame(data_array, schema_array)\n",
    "\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfdf011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| SubId|\n",
      "+---+------+\n",
      "|  A|[1, 2]|\n",
      "|  B|[3, 4]|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "data_array = [{'id':'A', 'SubId':[1,2]}, {'id':'B','SubId': [3,4]}]\n",
    "\n",
    "schema_array = StructType([\n",
    "    StructField(name = 'id', dataType = StringType()),\n",
    "     StructField(name = 'SubId', dataType = ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "df_array = spark.createDataFrame(data_array, schema_array)\n",
    "\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7fff8",
   "metadata": {},
   "source": [
    "#### Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f634fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+\n",
      "| Id|      Skills|     ArraySkils|\n",
      "+---+------------+---------------+\n",
      "|  1|Java, Python|[Java,  Python]|\n",
      "|  2|    Spark,SF|    [Spark, SF]|\n",
      "+---+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, array_contains\n",
    "\n",
    "data_array_01 = [(1, 'Java, Python'), (2, 'Spark,SF')]\n",
    "schema_array_01 = ['Id', 'Skills']\n",
    "\n",
    "df_array_01 = spark.createDataFrame(data_array_01, schema_array_01)\n",
    "\n",
    "df_array_01 = df_array_01.withColumn('ArraySkils', split(col('Skills'), ','))\n",
    "\n",
    "df_array_01.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4bc2",
   "metadata": {},
   "source": [
    "#### array_contains function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array_01.withColumn('ArrayContains', array_contains(col('ArraySkils'), 'Java')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840acf32",
   "metadata": {},
   "source": [
    "### Map Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52471d6f",
   "metadata": {},
   "source": [
    "Let's explore the Map data type and some functions of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, MapType\n",
    "\n",
    "df_map_data = [('Akshay', {'Height': '6', 'Hair': 'Brown'}), ('Rohan', {'Height': '6.5', 'Hair': 'Black'})]\n",
    "\n",
    "df_map_schema = StructType([\n",
    "    StructField(name='Name', dataType=StringType()),\n",
    "    StructField(name='Properties', dataType=MapType(StringType(),StringType()))\n",
    "])\n",
    "\n",
    "df_Map = spark.createDataFrame(df_map_data, df_map_schema)\n",
    "\n",
    "df_Map.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847f2bc",
   "metadata": {},
   "source": [
    "1. Accesing the values from Map and adding the values into new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b107fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Map = df_Map.withColumn('Hair', df_Map.Properties['Hair'])\n",
    "\n",
    "df_Map.show(truncate=False)\n",
    "\n",
    "df_Map = df_Map.withColumn('Height', df_Map.Properties['Height'])\n",
    "\n",
    "df_Map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9122d",
   "metadata": {},
   "source": [
    "2. Accesing the Key and values from Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5089403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import map_keys, map_values\n",
    "\n",
    "df_Map.drop(col('Keys'))\n",
    "\n",
    "df_Map_Keys = df_Map.withColumn('Keys', map_keys(df_Map.Properties))\n",
    "\n",
    "df_Map_Keys.show(truncate=False)\n",
    "\n",
    "df_Map_Values = df_Map.withColumn('Values', map_values(df_Map.Properties))\n",
    "\n",
    "df_Map_Values.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7cef7",
   "metadata": {},
   "source": [
    "## Exploring Condition and Functions of PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c1b13",
   "metadata": {},
   "source": [
    "### 01 When Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65474116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_when_prac_data = [ (2,'Akshay', 'M', 2000), (3, 'Akshata','F',3000), (4, 'Rohan', 'M', 6000)]\n",
    "\n",
    "df_when_prac_schema = ['Id', 'Name', 'Gender', 'Salary']\n",
    "\n",
    "df_when_prac = spark.createDataFrame(df_when_prac_data, df_when_prac_schema)\n",
    "\n",
    "df_when_prac.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_when_prac.select(\n",
    "    df_when_prac.Name, \n",
    "    when(condition=df_when_prac.Gender=='M', value='Male')\\\n",
    "    .when(condition=df_when_prac.Gender=='F', value='Female')\\\n",
    "    .otherwise('Male').alias('Gender')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772988c",
   "metadata": {},
   "source": [
    "### 02 Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_when_prac.select(df_when_prac.Id.alias('Emp_ID'), df_when_prac.Name.alias('Emp_Name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55ac97",
   "metadata": {},
   "source": [
    "### 03 Asc & Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_when_prac.sort(df_when_prac.Id.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45985e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_when_prac.sort(df_when_prac.Id.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaa62d",
   "metadata": {},
   "source": [
    "### 04 Like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16292b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_when_prac.filter(df_when_prac.Name.like('A%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749cb2d",
   "metadata": {},
   "source": [
    "### 05 Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_prac_data = [(1,'Akshay'), (1,'Akshay'), (2,'Amarnath'), (3,'Adarsh'), (4,'Ganesh')]\n",
    "\n",
    "df_dist_prac_schema = ['Id', 'Names']\n",
    "\n",
    "df_dist_prac = spark.createDataFrame(df_dist_prac_data, df_dist_prac_schema)\n",
    "\n",
    "df_dist_prac.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daceaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_prac.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee4cd4",
   "metadata": {},
   "source": [
    "## Group By and Aggregating Function\n",
    "\n",
    "    Exploring more about Group by function and Aggregating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c70bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_prac_data = [\n",
    "    (\"Alice\", \"Engineering\", 5000),\n",
    "    (\"Bob\", \"Engineering\", 6000),\n",
    "    (\"Charlie\", \"Sales\", 4000),\n",
    "    (\"David\", \"Marketing\", 7000),\n",
    "    (\"Eva\", \"Sales\", 4500),\n",
    "    (\"Frank\", \"Marketing\", 5500),\n",
    "]\n",
    "\n",
    "df_grp_prac_schema = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df_grp_prac = spark.createDataFrame(df_grp_prac_data, df_grp_prac_schema)\n",
    "\n",
    "df_grp_prac.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f638c2e",
   "metadata": {},
   "source": [
    "    1. Counting the Number of Employess working in each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_prac.groupBy(df_grp_prac.Department).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e6560",
   "metadata": {},
   "source": [
    "    2. Finding the Max salary of each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12812198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_prac.groupBy(df_grp_prac.Department).max('Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3b1b3",
   "metadata": {},
   "source": [
    "    2. Finding the Min salary of each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_prac.groupBy(df_grp_prac.Department).min('Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baa49c",
   "metadata": {},
   "source": [
    "    3. Finding the Total of salary of each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_prac.groupBy(df_grp_prac.Department).sum('Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135615c",
   "metadata": {},
   "source": [
    "If we want to utilize multiple aggregate functions within a single DataFrame, we can employ the `agg` function to compute these aggregate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, max, min, sum\n",
    "\n",
    "df_grp_prac.groupBy(df_grp_prac.Department).agg(\n",
    "            count('*').alias('Number_of_emp'),\n",
    "            sum('Salary').alias('Total_salary'),   \n",
    "            min('Salary').alias('Min_Salary'), \n",
    "            max('Salary').alias('Max_Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3f19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkEnv",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
